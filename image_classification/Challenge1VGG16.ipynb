{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copia di Copia di train_TL_VGG_FineTuning_1.ipynb","provenance":[],"collapsed_sections":[]},"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nteract":{"version":"nteract-front-end@1.0.0"}},"cells":[{"cell_type":"code","metadata":{"id":"C0rOao7CUkzO"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCUQzWYvy9cL"},"source":["%cd /gdrive/My Drive/Datasets/homework1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckFhCR3jFgSA"},"source":["!mkdir ~/leaves_hw\n","!cp ./dataset_70_15_15.zip ~/leaves_hw/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCgwD23j1p1a","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["%cd ~/leaves_hw\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637242216074},"id":"5BgpyXaSXFxR"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import random\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils import class_weight\n","from PIL import Image\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637242216632},"id":"0LRkmPHufyMZ"},"source":["# Random seed for reproducibility\n","seed = 57\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245227406},"id":"O46w_DgpyDxu"},"source":["#Dati\n","input_shape = (256, 256, 3)\n","epochs = 400\n","n_classes = 14"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637076822764},"id":"QGpKkMV6f3_w","scrolled":true},"source":["# Load the dataset to be used for classification\n","\n","# ################################\n","# ################################\n","# ################################\n","# ################################\n","\n","!unzip dataset_70_15_15.zip\n","\n","# ################################\n","# ################################\n","# ################################\n","# ################################\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637242245478},"id":"DTpdFpTNf8dj"},"source":["dataset_dir = './dataset'\n","training_dir = os.path.join(dataset_dir, 'training')\n","validation_dir = os.path.join(dataset_dir, 'validation')\n","test_dir = os.path.join(dataset_dir, 'test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637242247475},"id":"Vv-5Cyr8g2YX"},"source":["# # Plot example images from dataset\n","\n","# # lables in lexicografial order # index\n","lables = []\n","\n","f = open(\"./dataset/classes.csv\")\n","\n","lines = f.readlines()\n","\n","for line in lines:\n","\tlables.append(line.strip())\n"," \n","print(lables)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245170345},"id":"hvJOx3tVtx3j"},"source":["from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","\n","\n","def add_noise(img):\n","    '''Add random noise to an image'''\n","#     img = img.astype('float64')\n","    VARIABILITY = 5\n","    deviation = VARIABILITY*random.random()\n","    noise = np.random.normal(0, deviation, img.shape)\n","    img += noise\n","    np.clip(img, 0., 255.)\n","    return img\n","\n","def image_prep(img):\n","  img = preprocess_input(img)\n","  img = add_noise(img)\n","  \n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245171755},"id":"7NHUXbNwhthB"},"source":["# Images are divided into folders, one for each class. \n","# If the images are organized in such a way, we can exploit the \n","# ImageDataGenerator to read them from disk.\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_data_generator = ImageDataGenerator(\n","                          rotation_range=180, \n","                          height_shift_range=10,\n","                          width_shift_range=10,\n","                          zoom_range=0.7,\n","                          horizontal_flip=True,\n","                          vertical_flip=True,\n","                          #brightness_range=(-0.05,0.05),\n","                          fill_mode='nearest',\n","                          preprocessing_function=image_prep,\n","#                          rescale=1/255.\n","                     )\n","\n","valid_data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input\n",")\n","test_data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input\n",")\n","\n","train_gen = train_data_generator.flow_from_directory(\n","    directory=training_dir, # the directory\n","    # target_size=(408,408), # the size I want for the images (it resize the images to that one) - we use the original one\n","    target_size=(256,256), # the size I want for the images (it resize the images to that one) - we use the original one\n","    color_mode='rgb', # use the tree channels for the colors\n","    classes=lables, # default is None (this allows to decide from the name of the classes which is the integer to be associated to this class)\n","                  #   with None, it will read the folder name, and order it in lexicographical order\n","                  #   classes=lables    ## will give the integer associated to the classes as lable!\n","    batch_size=20,\n","    shuffle=True, # to shuffle the data at the end of every epoch \n","    seed=seed\n",")\n","\n","valid_gen = valid_data_generator.flow_from_directory(\n","    directory=validation_dir, # the directory\n","    target_size=(256,256), # the size I want for the images (it resize the images to that one) - we use the original one\n","    color_mode='rgb', # use the tree channels for the colors\n","    classes=lables, # default is None (this allows to decide from the name of the classes which is the integer to be associated to this class)\n","                  #   with None, it will read the folder name, and order it in lexicographical order\n","                  #   classes=lables    ## will give the integer associated to the classes as lable!\n","    batch_size=20,\n","    shuffle=False, # to shuffle the data at the end of every epoch \n","                   # shuffling is not important in the validation set!\n","    seed=seed\n",")\n","\n","test_gen = test_data_generator.flow_from_directory(\n","    directory=test_dir, # the directory\n","    target_size=(256,256), # the size I want for the images (it resize the images to that one) - we use the original one\n","    color_mode='rgb', # use the tree channels for the colors\n","    classes=lables, # default is None (this allows to decide from the name of the classes which is the integer to be associated to this class)\n","                  #   with None, it will read the folder name, and order it in lexicographical order\n","                  #   classes=lables    ## will give the integer associated to the classes as lable!\n","    batch_size=8,\n","    shuffle=False, # to shuffle the data at the end of every epoch \n","    seed=seed\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7mKeQZ9nB2T"},"source":["from sklearn.utils import class_weight\n","\n","\n","class_weights = class_weight.compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(train_gen.labels),\n","                                        y = train_gen.labels                                                    \n","                                    )\n","\n","print(class_weights)\n","\n","minimum = 0\n","minimum = min(class_weights)\n","\n","cv = {}\n","for i in range(14):\n","  cv[i] = class_weights[i]/minimum\n","\n","print(cv)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245172157},"id":"2Z7jw1rbzifh"},"source":["def get_next_batch(generator):\n","  batch = next(generator) # get the next batch\n","\n","  image = batch[0]    # image   X\n","  target = batch[1]   # target  t\n","\n","  print(\"(Input) image shape:\", image.shape)\n","    # each image variable is a batch of 8 samples\n","\n","  print(\"Target shape:\",target.shape)\n","    # it is the one-hot encoding\n","    # this is why we have the shape of target (8,21)\n","    #     because it is converted to one-hot encoding\n","\n","  # Visualize only the first sample\n","  image = image[0]\n","  target = target[0]\n","  target_idx = np.argmax(target)\n","  print()\n","  print(\"Categorical label:\", target)\n","  print(\"Label:\", target_idx)\n","  print(\"Class name:\", lables[target_idx])\n","  fig = plt.figure(figsize=(6, 4))\n","  plt.imshow(np.uint8(image))\n","\n","  return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245183078},"id":"HOpTeEf4zlKb"},"source":["# Get a sample from dataset and show info\n","_ = get_next_batch(train_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245186469},"id":"IbIkV2d7lyIm"},"source":["# Model used:\n","def build_model(input_shape):\n","    global n_classes\n","\n","    supernet = tfk.applications.VGG16(\n","      include_top=False, # remove the classifier from VGG (keep only the classifies)\n","      weights=\"imagenet\", # takes the weight already trained\n","      input_shape=input_shape # shape\n","    )   \n","\n","    supernet.trainable = False  # to not fit the VGG (the supernet)\n","\n","\n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","\n","\n","    sup = supernet(input_layer)\n","\n","\n","    flattening_layer = tfkl.Flatten(name='Flatten')(sup)\n","\n","    \n","    dp6 = tfkl.Dropout(0.3, seed=seed)(flattening_layer)\n","    classifier_layer_6 = tfkl.Dense(units=128, name='Classifier6', kernel_initializer=tfk.initializers.GlorotUniform(seed), activation='relu')(dp6)\n","    \n","\n","    output_layer = tfkl.Dense(units=n_classes, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='Output')(classifier_layer_6)\n","\n","\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","    optimizer=tfk.optimizers.Adam(\n","            learning_rate=0.0005\n","    )\n","\n","    # Compile the model\n","    model.compile(\n","        loss=tfk.losses.CategoricalCrossentropy(), \n","        optimizer=optimizer, \n","        metrics='accuracy'\n","    )\n","\n","    # Return the model\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245274682},"id":"SWgkfJbjmE1Y"},"source":["\n","# Utility function to create folders and callbacks for training\n","from datetime import datetime\n","\n","def create_folders_and_callbacks(model_name):\n","\n","  exps_dir = os.path.join('logs_and_results')\n","  if not os.path.exists(exps_dir):\n","      os.makedirs(exps_dir)\n","\n","  now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","  if not os.path.exists(exp_dir):\n","      os.makedirs(exp_dir)\n","      \n","  callbacks = []\n","\n","  # Model checkpoint\n","  # ----------------\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","  if not os.path.exists(ckpt_dir):\n","      os.makedirs(ckpt_dir)\n","\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp'),\n","                                                     save_weights_only=False,\n","                                                     save_best_only=False)\n","  # callbacks.append(ckpt_callback)\n","\n","  # Visualize Learning on Tensorboard\n","  # ---------------------------------\n","  tb_dir = os.path.join(exp_dir, 'tb_logs')\n","  if not os.path.exists(tb_dir):\n","      os.makedirs(tb_dir)\n","      \n","  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                               profile_batch=0,\n","                                               histogram_freq=1)\n","  # callbacks.append(tb_callback)\n","\n","\n","  # -------------------- #\n","  # CHANGE learning_rate #\n","  # -------------------- #\n","  from keras.callbacks import LearningRateScheduler\n","\n","  # This is a sample of a scheduler I used in the past\n","  def lr_scheduler(epoch, lr):\n","    if epoch > 4:\n","      lr = lr * tf.math.exp(-0.1)\n","    return lr\n","\n","  change_lr_callback = LearningRateScheduler(lr_scheduler, verbose=1)\n","  callbacks.append(change_lr_callback)\n","\n","  # Early Stopping\n","  # --------------\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n","  callbacks.append(es_callback)\n","\n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637245276885},"id":"u37RIkVfmK7Q"},"source":["# Build model (for NO augmentation training)\n","model = build_model(input_shape)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637094258113},"id":"TJA30f08mMnd"},"source":["# Creathttps://accounts.google.com/o/oauth2/approval/v2/approvalnativeapp?auto=false&response=code%3D4%2F1AX4XfWgtWkgsNgsKy4bK1aIMgpqWjN2OMh4-oTu0pSKreeCuYpaNqoHE-C4%26scope%3Demail%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.activity.readonly%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fexperimentsandconfigs%2520https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fphotos.native%2520openid%26authuser%3D1%26prompt%3Dconsent&hl=it&approvalCode=4%2F1AX4XfWgtWkgsNgsKy4bK1aIMgpqWjN2OMh4-oTu0pSKreeCuYpaNqoHE-C4e folders and callbacks and fit\n","callbacks = create_folders_and_callbacks(model_name='CNN_')\n","\n","# Train the model\n","history = model.fit(\n","    x = train_gen,\n","    epochs = epochs,\n","    validation_data = valid_gen,\n","    class_weight = cv,\n","    callbacks = callbacks,\n",").history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637107808068},"id":"ASaIMrbSzy1d"},"source":["model.save(\"/gdrive/My Drive/Datasets/homework1/models_VGG_TL_2___/train_TL_VGG_1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YJIaa9L5QZp"},"source":["model.save(\"/gdrive/My Drive/Datasets/homework1/models_VGG_TL_2___/train_TL_VGG_1.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1637220677811},"id":"TH1vLsNGn1zh"},"source":["model = tfk.models.load_model(\"/gdrive/My Drive/Datasets/homework1/models_VGG_TL_2___/train_TL_VGG_1.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LASr86Q6-lY"},"source":[".\n",".\n",".\n","FINE TUNING\n",".\n",".\n","."]},{"cell_type":"code","metadata":{"id":"FUKaFVm76-Pu"},"source":["model.get_layer('vgg16').trainable = True\n","\n","for i, layer in enumerate(model.get_layer('vgg16').layers[:14]):\n","  layer.trainable=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o727pJg77F7m"},"source":["model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"by_cLHhpldTK"},"source":["\n","# Utility function to create folders and callbacks for training\n","from datetime import datetime\n","\n","def create_folders_and_callbacks_ft(model_name):\n","\n","  exps_dir = os.path.join('logs_and_results')\n","  if not os.path.exists(exps_dir):\n","      os.makedirs(exps_dir)\n","\n","  now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","  if not os.path.exists(exp_dir):\n","      os.makedirs(exp_dir)\n","      \n","  callbacks = []\n","\n","  # Model checkpoint\n","  # ----------------\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","  if not os.path.exists(ckpt_dir):\n","      os.makedirs(ckpt_dir)\n","\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp'),\n","                                                     save_weights_only=False,\n","                                                     save_best_only=False)\n","  # callbacks.append(ckpt_callback)\n","\n","  # Visualize Learning on Tensorboard\n","  # ---------------------------------\n","  tb_dir = os.path.join(exp_dir, 'tb_logs')\n","  if not os.path.exists(tb_dir):\n","      os.makedirs(tb_dir)\n","      \n","  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                               profile_batch=0,\n","                                               histogram_freq=1)\n","  # callbacks.append(tb_callback)\n","\n","\n","  # -------------------- #\n","  # CHANGE learning_rate #\n","  # -------------------- #\n","  from keras.callbacks import LearningRateScheduler\n","\n","  # This is a sample of a scheduler I used in the past\n","  def lr_scheduler(epoch, lr):\n","    if epoch > 4:\n","      lr = lr * tf.math.exp(-0.1)\n","    return lr\n","\n","  change_lr_callback = LearningRateScheduler(lr_scheduler, verbose=1)\n","  callbacks.append(change_lr_callback)\n","\n","  # Early Stopping\n","  # --------------\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","  callbacks.append(es_callback)\n","\n","  return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2lvaaz97ITI"},"source":["callbacks = create_folders_and_callbacks_ft(model_name='CNN_FT_')\n","\n","# Fine-tune the model\n","history = model.fit(\n","    x = train_gen,\n","    epochs = epochs,\n","    validation_data = valid_gen,\n","    class_weight = cv,\n","    callbacks = callbacks\n",").history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x0XyVzrldTL"},"source":["model.save(\"/gdrive/My Drive/Datasets/homework1/models_VGG_TL_2___/train_TL_VGG_1_TL_1.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeGVvMZ_tx3r","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":[""],"execution_count":null,"outputs":[]}]}